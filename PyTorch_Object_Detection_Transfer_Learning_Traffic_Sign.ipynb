{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0X_28bIfLrra",
        "kjsQoQ2bL-lu",
        "8rEBBuLJMBGa",
        "zmpf8SfiaP38"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://www.kaggle.com/datasets/andrewmvd/road-sign-detection"
      ],
      "metadata": {
        "id": "rfml5rIPRdxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prerequisites**"
      ],
      "metadata": {
        "id": "0X_28bIfLrra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting GitHub for dataset if previously downloaded and transferred to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPzA4v0wUAoz",
        "outputId": "bf1aede3-9428-47bf-90fb-88678a9564c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scripts containing some prerequisite functions\n",
        "! wget \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\"\n",
        "! wget \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\"\n",
        "! wget \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\"\n",
        "! wget \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\"\n",
        "! wget \"https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\""
      ],
      "metadata": {
        "id": "A9tdedTdLBiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prerequisite libraries\n",
        "!pip install torch torchvision albumentations\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "vWKki29E5PPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting XML annotation files to txt files (suitable for YOLO)\n",
        "# Creat a folder named \"ann_YOLO\" in \"Traffic_Sign_2\" first.\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Mapping categories to class IDs\n",
        "categories = {'trafficlight': 0, 'stop': 1, 'speedlimit': 2, 'crosswalk': 3}\n",
        "\n",
        "def convert(size, box):\n",
        "    dw = 1. / size[0]\n",
        "    dh = 1. / size[1]\n",
        "    x = (box[0] + box[1]) / 2.0\n",
        "    y = (box[2] + box[3]) / 2.0\n",
        "    w = box[1] - box[0]\n",
        "    h = box[3] - box[2]\n",
        "    return x * dw, y * dh, w * dw, h * dh\n",
        "\n",
        "def convert_annotation(xml_file, output_dir):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    size = root.find('size')\n",
        "    width = int(size.find('width').text)\n",
        "    height = int(size.find('height').text)\n",
        "\n",
        "    output_file = os.path.join(output_dir, os.path.basename(xml_file).replace('.xml', '.txt'))\n",
        "    with open(output_file, 'w') as out_file:\n",
        "        for obj in root.iter('object'):\n",
        "            cls = obj.find('name').text\n",
        "            if cls not in categories:\n",
        "                continue\n",
        "            cls_id = categories[cls]\n",
        "            xml_box = obj.find('bndbox')\n",
        "            b = (float(xml_box.find('xmin').text), float(xml_box.find('xmax').text),\n",
        "                 float(xml_box.find('ymin').text), float(xml_box.find('ymax').text))\n",
        "            bb = convert((width, height), b)\n",
        "            out_file.write(f\"{cls_id} \" + \" \".join(map(str, bb)) + '\\n')\n",
        "\n",
        "\n",
        "# Convert all annotations\n",
        "input_dir = \"/content/drive/MyDrive/Traffic_Sign_2/annotations\"\n",
        "output_dir = \"/content/drive/MyDrive/Traffic_Sign_2/ann_YOLO\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for file in os.listdir(input_dir):\n",
        "    if file.endswith(\".xml\"):\n",
        "        convert_annotation(os.path.join(input_dir, file), output_dir)\n"
      ],
      "metadata": {
        "id": "x1T9yRY1Gj62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset to train and test\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Set paths\n",
        "data_dir = \"/content/drive/MyDrive/Traffic_Sign_2\"\n",
        "img_dir = os.path.join(data_dir, \"images\")\n",
        "ann_dir = os.path.join(data_dir, \"ann_YOLO\")\n",
        "\n",
        "# Create train/test directories\n",
        "train_img_dir = os.path.join(data_dir, \"YOLO/images/train\")\n",
        "train_ann_dir = os.path.join(data_dir, \"YOLO/labels/train\")\n",
        "test_img_dir = os.path.join(data_dir, \"YOLO/images/val\")\n",
        "test_ann_dir = os.path.join(data_dir, \"YOLO/labels/val\")\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(train_img_dir, exist_ok=True)\n",
        "os.makedirs(train_ann_dir, exist_ok=True)\n",
        "os.makedirs(test_img_dir, exist_ok=True)\n",
        "os.makedirs(test_ann_dir, exist_ok=True)\n",
        "\n",
        "# Get sorted lists of images and annotations\n",
        "images = sorted(os.listdir(img_dir))\n",
        "annotations = sorted(os.listdir(ann_dir))\n",
        "\n",
        "# Check that each image has a corresponding annotation\n",
        "assert len(images) == len(annotations), \"Mismatch between image and annotation counts\"\n",
        "\n",
        "# Combine images and annotations into pairs\n",
        "data_pairs = list(zip(images, annotations))\n",
        "\n",
        "# Shuffle and split the data\n",
        "random.seed(42)  # For identical randomness for every run\n",
        "random.shuffle(data_pairs)\n",
        "split_idx = int(0.8 * len(data_pairs))\n",
        "\n",
        "train_pairs = data_pairs[:split_idx]\n",
        "test_pairs = data_pairs[split_idx:]\n",
        "\n",
        "# Move files into the corresponding directories\n",
        "for img_name, ann_name in train_pairs:\n",
        "    shutil.copy(os.path.join(img_dir, img_name), os.path.join(train_img_dir, img_name))\n",
        "    shutil.copy(os.path.join(ann_dir, ann_name), os.path.join(train_ann_dir, ann_name))\n",
        "\n",
        "for img_name, ann_name in test_pairs:\n",
        "    shutil.copy(os.path.join(img_dir, img_name), os.path.join(test_img_dir, img_name))\n",
        "    shutil.copy(os.path.join(ann_dir, ann_name), os.path.join(test_ann_dir, ann_name))\n",
        "\n",
        "print(\"Dataset split complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7KfX5m-XzRu",
        "outputId": "f259f658-e67b-4b33-c925-c2378ef1b27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loader**"
      ],
      "metadata": {
        "id": "kjsQoQ2bL-lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the custom dataloader function\n",
        "\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class Traffic_sign_dataset_2_custom_function(Dataset):\n",
        "\n",
        "  def __init__(self, img_dir, ann_dir, transform = None):\n",
        "    self.img_dir = img_dir\n",
        "    self.ann_dir = ann_dir\n",
        "    self.transform = transform\n",
        "    self.imgs = sorted(os.listdir(img_dir))\n",
        "    self.anns = sorted(os.listdir(ann_dir))\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "\n",
        "  def anno_parser(self, ann_path):\n",
        "    tree = ET.parse(ann_path)\n",
        "    root = tree.getroot()\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    string_lable_to_numerical = {\n",
        "        'trafficlight': 0,\n",
        "        'stop': 1,\n",
        "        'speedlimit': 2,\n",
        "        'crosswalk': 3\n",
        "    }\n",
        "\n",
        "    for obj in root.findall(\"object\"):\n",
        "      label = obj.find(\"name\").text\n",
        "      label_to_number = string_lable_to_numerical[label]  # You should apply opposite of this in the evaluation\n",
        "      labels.append(label_to_number)\n",
        "      bbox = obj.find(\"bndbox\")\n",
        "      xmin = int(bbox.find(\"xmin\").text)\n",
        "      ymin = int(bbox.find(\"ymin\").text)\n",
        "      xmax = int(bbox.find(\"xmax\").text)\n",
        "      ymax = int(bbox.find(\"ymax\").text)\n",
        "      boxes.append([xmin-1, ymin-2, xmax, ymax])\n",
        "      numerical_and_torch_labels = torch.tensor(labels, dtype = torch.int64)\n",
        "    return {\"boxes\": torch.tensor(boxes, dtype = torch.float32), \"labels\": numerical_and_torch_labels}\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
        "    ann_path = os.path.join(self.ann_dir, self.anns[idx])\n",
        "\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    # imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
        "    # image = (image - imagenet_stats[0]) / imagenet_stats[1]\n",
        "    target = self.anno_parser(ann_path)\n",
        "\n",
        "    # sections just for the COCO evaluator\n",
        "    target['image_id'] = int(torch.tensor([idx]))\n",
        "    boxes = target[\"boxes\"]\n",
        "    area = [(box[2] - box[0]) * (box[3] - box[1]) for box in boxes]\n",
        "    target['area'] = torch.tensor(area, dtype=torch.float32)\n",
        "    iscrowd = torch.zeros((len(boxes), ), dtype = torch.int64)\n",
        "    target['iscrowd'] = iscrowd\n",
        "\n",
        "    if self.transform is not None:\n",
        "        image, target = self.transform(image, target)\n",
        "    return image, target\n"
      ],
      "metadata": {
        "id": "1Sc09w1-M6GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation functions\n",
        "\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "import random\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "\n",
        "# Changing the contrast of the images\n",
        "class ContrastEnhanceTransform:\n",
        "    def __init__(self, contrast_factor=1):\n",
        "        self.contrast_factor = contrast_factor\n",
        "    def __call__(self, img):\n",
        "        # Enhance contrast using PIL's ImageEnhance\n",
        "        enhancer = ImageEnhance.Contrast(img)\n",
        "        return enhancer.enhance(self.contrast_factor)\n",
        "\n",
        "\n",
        "class CustomTransform:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.resize = T.Resize(size)\n",
        "\n",
        "        self.image_only_transforms = T.Compose([\n",
        "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            ContrastEnhanceTransform(contrast_factor=1.2)\n",
        "        ])\n",
        "\n",
        "        self.to_tensor = T.ToTensor()\n",
        "        # self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        # # Random horizontal flip - Uncomment if you'd like to use it\n",
        "        # if random.random() < 0.5:\n",
        "        #     image = F.hflip(image)\n",
        "        #     boxes = target[\"boxes\"]\n",
        "        #     w = image.width  # Original image width\n",
        "        #     boxes[:, [0, 2]] = w - boxes[:, [2, 0]]  # Flip x-coordinates\n",
        "        #     target[\"boxes\"] = boxes\n",
        "\n",
        "        # # Apply image-only transformations\n",
        "        # image = self.image_only_transforms(image)\n",
        "\n",
        "        # Resize image\n",
        "        w_orig, h_orig = image.size\n",
        "        image = self.resize(image)\n",
        "        w_new, h_new = self.size\n",
        "\n",
        "        # Calculate the scale factors\n",
        "        scale_x = w_new / w_orig\n",
        "        scale_y = h_new / h_orig\n",
        "\n",
        "        # Resize bounding boxes\n",
        "        boxes = target[\"boxes\"]\n",
        "        boxes[:, [0, 2]] *= scale_x\n",
        "        boxes[:, [1, 3]] *= scale_y\n",
        "        target[\"boxes\"] = boxes\n",
        "\n",
        "        # Convert to tensor\n",
        "        image = self.to_tensor(image)\n",
        "        # image = self.normalize(image)\n",
        "\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "ur5cbGfXW8ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting data loaders\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = CustomTransform(size = (640, 640))\n",
        "\n",
        "\n",
        "train_dataset = Traffic_sign_dataset_2_custom_function(\n",
        "    img_dir = '/content/drive/MyDrive/Traffic_Sign_2/img_train',\n",
        "    ann_dir = '/content/drive/MyDrive/Traffic_Sign_2/ann_train',\n",
        "    transform = transform\n",
        ")\n",
        "\n",
        "test_dataset = Traffic_sign_dataset_2_custom_function(\n",
        "    img_dir = '/content/drive/MyDrive/Traffic_Sign_2/img_test',\n",
        "    ann_dir = '/content/drive/MyDrive/Traffic_Sign_2/ann_test',\n",
        "    transform = transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle = True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_loader = DataLoader(test_dataset, batch_size = 4, shuffle = False, collate_fn=lambda x: tuple(zip(*x)))\n"
      ],
      "metadata": {
        "id": "HwylL6GCXCPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualization**"
      ],
      "metadata": {
        "id": "8rEBBuLJMBGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def visualizing(img, annotation, ax):\n",
        "    image = img.permute(1, 2, 0).numpy()\n",
        "    ax.imshow(image)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    boxes = annotation['boxes']\n",
        "    labels = annotation['labels']\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        x1, y1, x2, y2 = boxes[i].int()\n",
        "\n",
        "        rec = patches.Rectangle(\n",
        "            (x1, y1),\n",
        "            x2 - x1,\n",
        "            y2 - y1,\n",
        "            linewidth=1,\n",
        "            edgecolor='r',\n",
        "            facecolor='none'\n",
        "        )\n",
        "        ax.add_patch(rec)\n",
        "\n",
        "        label = labels[i]\n",
        "        string_label_to_numerical = {\n",
        "            '0': 'traficlight',\n",
        "            '1': 'stop',\n",
        "            '2': 'speedlimit',\n",
        "            '3': 'crosswalk'\n",
        "        }\n",
        "\n",
        "        label_string = string_label_to_numerical[f'{label}']\n",
        "\n",
        "        ax.text(\n",
        "            x1,\n",
        "            y1 - 10,\n",
        "            f\"{label_string}\",\n",
        "            color='red',\n",
        "            fontsize=8,\n",
        "            weight='bold'\n",
        "        )\n"
      ],
      "metadata": {
        "id": "EdEXGdo5AwVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, targets = next(iter(train_loader))\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    image, target = images[i], targets[i]\n",
        "    visualizing(image, target, ax)\n"
      ],
      "metadata": {
        "id": "1zlEiBGiAf8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Configuration (Faster R-CNN & SSD)**"
      ],
      "metadata": {
        "id": "Mg8sMdEYMM8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the FPN network\n",
        "\n",
        "import warnings\n",
        "from typing import Callable, Dict, List, Optional, Union\n",
        "from torch import nn, Tensor\n",
        "from torchvision.ops import misc as misc_nn_ops\n",
        "from torchvision.ops.feature_pyramid_network import ExtraFPNBlock, FeaturePyramidNetwork, LastLevelMaxPool\n",
        "from torchvision import models\n",
        "from torchvision.models import mobilenet, resnet\n",
        "from torchvision.models._api import _get_enum_from_fn, WeightsEnum\n",
        "from torchvision.models._utils import handle_legacy_interface, IntermediateLayerGetter\n",
        "\n",
        "\n",
        "class BackboneWithFPN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: nn.Module,\n",
        "        return_layers: Dict[str, str],\n",
        "        in_channels_list: List[int],\n",
        "        out_channels: int,\n",
        "        extra_blocks: Optional[ExtraFPNBlock] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if extra_blocks is None:\n",
        "            extra_blocks = LastLevelMaxPool()\n",
        "\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "        self.fpn = FeaturePyramidNetwork(\n",
        "            in_channels_list=in_channels_list,\n",
        "            out_channels=out_channels,\n",
        "            extra_blocks=extra_blocks,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
        "        x = self.body(x)\n",
        "        permuted_feature_maps = {str(i): torch.permute(x[k], (0, 3, 1, 2)) for i, k in enumerate(x)}\n",
        "        x = self.fpn(permuted_feature_maps)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3H7FYFJD9oYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ******************************************************************************\n",
        "# resnet50 - a full object detection *******************************************\n",
        "# ******************************************************************************\n",
        "\n",
        "def get_model_FRCNNresnet50(num_classes):\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  return model\n",
        "\n",
        "\n",
        "# ******************************************************************************\n",
        "# resnet101 *********************************************************************\n",
        "# ******************************************************************************\n",
        "\n",
        "def get_model_resnet101(num_classes):\n",
        "    backbone = torchvision.models.resnet101(weights=\"DEFAULT\")\n",
        "    backbone_features = nn.Sequential(*list(backbone.children())[:-1])\n",
        "    backbone_features.out_channels = 2048\n",
        "    model = FasterRCNN(\n",
        "        backbone=backbone_features,\n",
        "        num_classes=num_classes,\n",
        "    )\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    model.roi_heads.box_predictor.cls_loss_func = lambda logits, targets: custom_loss_function(logits, targets, class_weights)\n",
        "    return model\n",
        "\n",
        "\n",
        "# ******************************************************************************\n",
        "# ResNet50 + fpn ***************************************************************\n",
        "# ******************************************************************************\n",
        "\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "def get_model_custom_resnet50fpn(num_classes):\n",
        "    # Load a pre-trained model for classification and return only the features\n",
        "    backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT').backbone\n",
        "    # backbone = resnet_fpn_backbone(backbone_name=\"resnet50\", pretrained=True)\n",
        "    backbone.out_channels = 256\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes = ((32, 64, 128, 256, 512),) * 5,\n",
        "        aspect_ratios=(0.5, 1.0, 2.0),  # Same aspect ratios for all levels\n",
        ")\n",
        "    # Define the region of interest (RoI) pooling\n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "        featmap_names=['0','1','2','3','pool'],  # Match the FPN feature map names\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "    # Create the model\n",
        "    model = FasterRCNN(\n",
        "        backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler\n",
        "    )\n",
        "    # Replace the classifier head\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "# ******************************************************************************\n",
        "# Swin-T + fpn (with 3 channel input)*******************************************\n",
        "# ******************************************************************************\n",
        "\n",
        "import numpy as np\n",
        "class SwinBackboneWithFPN3channel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = torchvision.models.swin_t(weights='DEFAULT').features\n",
        "        self.return_layers = {'1': '0', '3': '1', '5': '2', '7': '3'}\n",
        "        self.in_channels_list = [96, 192, 384, 768]\n",
        "        self.out_channels = 256\n",
        "        self.fpn = BackboneWithFPN(self.backbone,\n",
        "                                   self.return_layers,\n",
        "                                   self.in_channels_list,\n",
        "                                   self.out_channels)\n",
        "    def forward(self, x):\n",
        "        return self.fpn(x)\n",
        "\n",
        "def get_model_swin_t_fpn(num_classes):\n",
        "    backbone = SwinBackboneWithFPN3channel()\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes = ((32, 64, 128, 256, 512),) * 5,\n",
        "        aspect_ratios = ((0.5, 1.0, 2.0),) * 5    )\n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "        featmap_names = ['0', '1', '2', '3', '4'],\n",
        "        output_size = 7,\n",
        "        sampling_ratio = 2)\n",
        "    model = FasterRCNN(\n",
        "        backbone,\n",
        "        num_classes = num_classes,\n",
        "        rpn_anchor_generator = anchor_generator,\n",
        "        box_roi_pool=roi_pooler)\n",
        "    # Replace the classifier head\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    # To compensate for the class imbalance problem (If use a different dataset, then need use different valus here)\n",
        "    class_weights = torch.tensor([7.1871, 14.2714,  1.6061,  5.9464], device=device)\n",
        "    box_predictor.cls_loss_func = torch.nn.CrossEntropyLoss(weight = class_weights)\n",
        "    model.roi_heads.box_predictor = box_predictor\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ******************************************************************************\n",
        "# swin-t 512 1 channel (without FPN) *******************************************\n",
        "# ******************************************************************************\n",
        "\n",
        "class SwinBackboneWith1channel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = torchvision.models.swin_t(weights=\"DEFAULT\").features\n",
        "        # Modifying the first layer of the model to receive images with one channel\n",
        "        first_conv_layer = self.backbone[0][0]\n",
        "        new_first_conv_layer = nn.Conv2d(\n",
        "            in_channels = 1,  # The number of input image channels\n",
        "            out_channels = first_conv_layer.out_channels,\n",
        "            kernel_size = first_conv_layer.kernel_size,\n",
        "            stride = first_conv_layer.stride,\n",
        "            padding = first_conv_layer.padding,\n",
        "            bias = first_conv_layer.bias is not None        )\n",
        "        # Copy the original weights, average them to adapt to the single channel input\n",
        "        with torch.no_grad():\n",
        "            new_first_conv_layer.weight = nn.Parameter(\n",
        "                first_conv_layer.weight.mean(dim = 1, keepdim = True))\n",
        "            if first_conv_layer.bias is not None:\n",
        "                new_first_conv_layer.bias = nn.Parameter(first_conv_layer.bias)\n",
        "        # Replace the original conv layer with the new one\n",
        "        self.backbone[0][0] = new_first_conv_layer\n",
        "        self.out_channels = 768\n",
        "\n",
        "    def forward(self, x):\n",
        "      return torch.permute(self.backbone(x), (0, 3, 1, 2))\n",
        "\n",
        "def get_model_swin_t_1channel(num_classes):\n",
        "  backbone_swint = torchvision.models.swin_t(weights=\"DEFAULT\").features\n",
        "  anchor_generator = AnchorGenerator(\n",
        "      sizes = ((32, 64, 128, 256, 512),),\n",
        "      aspect_ratios = ((0.5, 1.0, 2.0),))\n",
        "  roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "      featmap_names = ['0'],\n",
        "      output_size = 7,\n",
        "      sampling_ratio = 2)\n",
        "  swin_backbone = SwinBackboneWith1channel()\n",
        "  model = FasterRCNN(\n",
        "      swin_backbone,\n",
        "      num_classes = 4,\n",
        "      rpn_anchor_generator = anchor_generator,\n",
        "      box_roi_pool = roi_pooler,\n",
        "      min_size = 224,\n",
        "      image_mean = [0.5],\n",
        "      image_std = [0.5]\n",
        "      )\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  return model\n",
        "\n",
        "\n",
        "# ******************************************************************************\n",
        "# Swin Transformer *************************************************************\n",
        "# ******************************************************************************\n",
        "\n",
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = torchvision.models.swin_t(weights=\"DEFAULT\").features\n",
        "        self.out_channels = 768\n",
        "    def forward(self, x):\n",
        "      return torch.permute(self.backbone(x), (0, 3, 1, 2))\n",
        "\n",
        "def get_model_swin_t(num_classes):\n",
        "  anchor_generator = AnchorGenerator(\n",
        "      sizes = ((32, 64, 128, 256, 512),),\n",
        "      aspect_ratios = ((0.5, 1.0, 2.0),))\n",
        "  roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "      featmap_names = ['0'],\n",
        "      output_size = 7,\n",
        "      sampling_ratio = 2)\n",
        "  swin_backbone = SwinTransformer()\n",
        "  model = FasterRCNN(\n",
        "      swin_backbone,\n",
        "      num_classes = 2,\n",
        "      rpn_anchor_generator = anchor_generator,\n",
        "      box_roi_pool = roi_pooler,\n",
        "      min_size = 300,\n",
        "      )\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "  box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "  # To compensate for the class imbalance problem (If use a different dataset, then need use different valus here)\n",
        "  class_weights = torch.tensor([7.1871, 14.2714,  1.6061,  5.9464], device=device)\n",
        "  box_predictor.cls_loss_func = torch.nn.CrossEntropyLoss(weight = class_weights)\n",
        "  model.roi_heads.box_predictor = box_predictor\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "# ******************************************************************************\n",
        "# ResNet 50 + imbalance data ***************************************************\n",
        "# ******************************************************************************\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Function to compute class weights\n",
        "def compute_class_weights(class_counts):\n",
        "    total = sum(class_counts)\n",
        "    num_classes = len(class_counts)\n",
        "    return torch.tensor([total / (num_classes * c) for c in class_counts], dtype=torch.float32)\n",
        "\n",
        "def get_model_FRCNNresnet50_with_weights(num_classes):\n",
        "    # Load the pre-trained Faster R-CNN model\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # Replace the classifier with a new one\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    class_weights = torch.tensor([7.1871, 14.2714,  1.6061,  5.9464], device=device)\n",
        "    box_predictor.cls_loss_func = torch.nn.CrossEntropyLoss(weight = class_weights)\n",
        "    model.roi_heads.box_predictor = box_predictor\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ******************************************************************************\n",
        "# SSD-VGG16 ********************************************************************\n",
        "# ******************************************************************************\n",
        "\n",
        "import torch\n",
        "from torchvision.models.detection.ssd import ssd300_vgg16\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "def get_model_SSD_VGG16(num_classes, imbalance=True):\n",
        "    model = ssd300_vgg16(pretrained=True)\n",
        "    model.head.classification_head.num_classes = num_classes\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "GjKYn6q6JSPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Testing**\n"
      ],
      "metadata": {
        "id": "2coPkkWGVq1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COCO Evaluation function\n",
        "\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset2(dataset):\n",
        "    coco = COCO()\n",
        "    coco.dataset = {\n",
        "        'images': [],\n",
        "        'annotations': [],\n",
        "        'categories': []\n",
        "    }\n",
        "\n",
        "    ann_id = 1\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        img, target = dataset[i]\n",
        "        temp_image_id = i\n",
        "        coco.dataset['images'].append({\n",
        "            'id': temp_image_id\n",
        "        })\n",
        "\n",
        "        boxes = target[\"boxes\"]\n",
        "        labels = target[\"labels\"]\n",
        "        image_id = target['image_id']\n",
        "        areas = target['area']\n",
        "        iscrowd = target['iscrowd']\n",
        "\n",
        "        for box, label, area, crowd in zip(boxes, labels, areas, iscrowd):\n",
        "            coco.dataset['annotations'].append({\n",
        "                'id': ann_id,  # Add unique ID for each annotation\n",
        "                'image_id': image_id,\n",
        "                'bbox': box.tolist(),\n",
        "                'category_id': int(label),\n",
        "                'area': float(area),\n",
        "                'iscrowd': int(crowd)\n",
        "            })\n",
        "\n",
        "            ann_id += 1\n",
        "    coco.createIndex()\n",
        "\n",
        "    return coco\n",
        "\n",
        "\n",
        "def evaluate_model2(model, data_loader, device, small, medium, large):\n",
        "    model.eval()\n",
        "    coco = get_coco_api_from_dataset2(data_loader.dataset)\n",
        "    coco.dataset['categories'] = [\n",
        "    {\"id\": 0, \"name\": \"trafficlight\"},\n",
        "    {\"id\": 1, \"name\": \"stop\"},\n",
        "    {\"id\": 2, \"name\": \"speedlimit\"},\n",
        "    {\"id\": 3, \"name\": \"crosswalk\"}\n",
        "    ]\n",
        "    coco_results = []\n",
        "    with torch.no_grad():\n",
        "      for images, targets in data_loader:\n",
        "          images = list(img.to(device) for img in images)\n",
        "          outputs = model(images)\n",
        "          for target, output in zip(targets, outputs):\n",
        "              image_id = int(target[\"image_id\"])\n",
        "              boxes = output[\"boxes\"].cpu()\n",
        "              scores = output[\"scores\"].cpu()\n",
        "              labels = output[\"labels\"].cpu()\n",
        "              for box, score, label in zip(boxes, scores, labels):\n",
        "                  coco_results.append({\n",
        "                      'image_id': image_id,\n",
        "                      'category_id': int(label),\n",
        "                      'bbox': box.tolist(),\n",
        "                      'score': float(score)\n",
        "                  })\n",
        "\n",
        "    coco_dt = coco.loadRes(coco_results)\n",
        "    coco_eval = COCOeval(coco, coco_dt, 'bbox')\n",
        "\n",
        "\n",
        "    coco_eval.params.maxDets = [1, 10, 20]\n",
        "    coco_eval.params.areaRng = [\n",
        "        [0, large],\n",
        "        [0, small],\n",
        "        [small + 0.1, medium],\n",
        "        [medium + 0.1, large]\n",
        "    ]\n",
        "\n",
        "    coco_eval.evaluate()\n",
        "    coco_eval.accumulate()\n",
        "    coco_eval.summarize()\n",
        "\n",
        "    precisions = coco_eval.eval['precision']\n",
        "    recalls = coco_eval.eval['recall']\n",
        "    iou_thresh = [0.5, 0.75, 0.95]\n",
        "    max_dets = [1, 10, 100]\n",
        "\n",
        "    area_indices = {\n",
        "        'all': 0,\n",
        "        'small': 1,\n",
        "        'medium': 2,\n",
        "        'large': 3\n",
        "    }\n",
        "\n",
        "    for area_name, area_idx in area_indices.items():\n",
        "        print(f\"\\nPrecision and Recall for area: {area_name.capitalize()}\")\n",
        "        for iou_idx, iou in enumerate(iou_thresh):\n",
        "            for det_idx, det in enumerate(max_dets):\n",
        "                precision = precisions[iou_idx, :, :, area_idx, det_idx]\n",
        "                recall = recalls[iou_idx, :, area_idx, det_idx]\n",
        "\n",
        "                # Calculating mAR and mAP\n",
        "                if precision.size > 0:\n",
        "                    ap = np.mean(precision[precision > -1]) # This excludes not calculated precisions (invalid values) before calculating the mean.\n",
        "                else:\n",
        "                    ap = -1\n",
        "                if recall.size > 0:\n",
        "                    ar = np.mean(recall[recall > -1])\n",
        "                else:\n",
        "                    ar = -1\n",
        "                print(f'@[ IoU={iou:.2f} | maxDets={det} ] -> AP: {ap:.3f}, AR: {ar:.3f}')\n",
        "\n",
        "# AP and AR\n",
        "    print(\"\\nDetailed results over all IoU thresholds [0.5:0.95] for each condition:\")\n",
        "    for area_name, area_idx in area_indices.items():\n",
        "        print(f\"\\nArea: {area_name.capitalize()}\")\n",
        "        for det_idx, det in enumerate(max_dets):\n",
        "            # Precision\n",
        "            precision = precisions[:, :, :, area_idx, det_idx]\n",
        "            if precision.size > 0:\n",
        "                ap = np.mean(precision[precision > -1])\n",
        "            else:\n",
        "                ap = -1\n",
        "            print(f'Precision @[ IoU=0.5:0.95 | maxDets={det} ] -> AP: {ap:.3f}')\n",
        "\n",
        "            # Recall\n",
        "            recall = recalls[:, :, area_idx, det_idx]\n",
        "            if recall.size > 0:\n",
        "                ar = np.mean(recall[recall > -1])\n",
        "            else:\n",
        "                ar = -1\n",
        "            print(f'Recall @[ IoU=0.5:0.95 | maxDets={det} ] -> AR: {ar:.3f}')\n",
        "\n",
        "\n",
        "    return coco_eval.stats\n",
        "\n",
        "\n",
        "# Enter the 3 percentiles of your object area here for the evaluation function\n",
        "# One needs to find these values specifically from their dataset\n",
        "percentile_small = 2080.00\n",
        "percentile_medium = 5939.66\n",
        "percentile_large = 110407.27\n"
      ],
      "metadata": {
        "id": "-_TSJwHQKLsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train_one_Epoch\n",
        "\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import torchvision.models.detection.mask_rcnn\n",
        "import utils\n",
        "from coco_eval import CocoEvaluator\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    header = f\"Epoch: [{epoch}]\"\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "        )\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "        loss_value = losses_reduced.item()\n",
        "        if not math.isfinite(loss_value):\n",
        "            sys.exit(1)\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            scaler.scale(losses).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "        # Ensure there is no conflict with 'loss' key\n",
        "        metric_logger.update(**{k: v for k, v in loss_dict_reduced.items() if k != 'loss'})\n",
        "        metric_logger.update(loss=losses_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "    return metric_logger\n",
        "\n"
      ],
      "metadata": {
        "id": "DNVarLCsK7n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "# The model will be evaluated after each epoch with the abovementioned COCO evaluator\n",
        "\n",
        "from engine import _get_iou_types, evaluate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# train on the GPU or on the CPU, if a GPU is not available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# get the model - Class+1\n",
        "model = get_model_SSD_VGG16(5)\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.0008,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=8,\n",
        "    gamma=0.1)\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq = 15)\n",
        "    lr_scheduler.step()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for images, targets in test_loader:\n",
        "        images = list(img.to(device) for img in images)\n",
        "        outputs = model(images)\n",
        "    evaluate_model2(model, test_loader, device, percentile_small, percentile_medium, percentile_large)\n",
        "\n"
      ],
      "metadata": {
        "id": "0DPzNWcoLHvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the trained model in a specified directory\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/traffic_sign_swin-t+FPN_3Channel_300_12epoch.pth')\n"
      ],
      "metadata": {
        "id": "8H1mUmHGpn_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}